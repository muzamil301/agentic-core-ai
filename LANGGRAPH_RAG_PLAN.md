# LangGraph + RAG Integration Plan: Chat Support App

## ğŸ“š Table of Contents
1. [Understanding RAG (Retrieval Augmented Generation)](#understanding-rag)
2. [How LangGraph Fits In](#how-langgraph-fits-in)
3. [Architecture Overview](#architecture-overview)
4. [Implementation Plan](#implementation-plan)
5. [File Structure](#file-structure)
6. [Dependencies](#dependencies)
7. [Step-by-Step Implementation](#step-by-step-implementation)
8. [Testing Strategy](#testing-strategy)

---

## ğŸ¯ Understanding RAG (Retrieval Augmented Generation)

### What is RAG?
RAG is a technique that enhances LLM responses by:
1. **Retrieving** relevant context from a knowledge base (your embeddings)
2. **Augmenting** the user's query with this context
3. **Generating** a response using both the query and retrieved context

### How RAG Works (Step-by-Step)

```
User Query: "What is my daily transaction limit?"
    â†“
1. QUERY EMBEDDING: Convert user query to embedding vector
    â†“
2. SEMANTIC SEARCH: Find similar embeddings in ChromaDB (vector similarity)
    â†“
3. RETRIEVAL: Get top-k most relevant documents (e.g., top 3)
    â†“
4. CONTEXT AUGMENTATION: Combine retrieved documents into context
    â†“
5. PROMPT CONSTRUCTION: Build prompt with:
   - System instructions
   - Retrieved context (knowledge base)
   - User query
    â†“
6. LLM GENERATION: Send to LLM (Ollama) to generate answer
    â†“
7. RESPONSE: Return natural language answer to user
```

### Why RAG?
- **Reduces Hallucination**: LLM uses actual data, not just training data
- **Up-to-date Information**: Knowledge base can be updated without retraining
- **Domain-Specific**: Works with your specific data (payment support)
- **Transparency**: Can cite sources from retrieved documents

---

## ğŸ”— How LangGraph Fits In

### What is LangGraph?
LangGraph is a framework for building **stateful, multi-actor applications** with LLMs. It uses graphs where:
- **Nodes** = Functions (retrieval, generation, etc.)
- **Edges** = Control flow (conditional routing)
- **State** = Shared data between nodes

### LangGraph in RAG Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangGraph State                       â”‚
â”‚  {query, context, retrieved_docs, response, history}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Graph Nodes                           â”‚
â”‚                                                          â”‚
â”‚  1. [Retrieve Node] â†’ Query ChromaDB                    â”‚
â”‚  2. [Format Context Node] â†’ Prepare prompt             â”‚
â”‚  3. [Generate Node] â†’ Call LLM                          â”‚
â”‚  4. [Validate Node] â†’ Check response quality            â”‚
â”‚  5. [Respond Node] â†’ Return to user                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits of Using LangGraph
- **Modular**: Each step is a separate node (easy to debug/modify)
- **Stateful**: Maintains conversation history
- **Conditional Logic**: Can route based on query type, confidence, etc.
- **Observable**: Easy to add logging, monitoring at each step
- **Extensible**: Add new nodes (e.g., fallback, human-in-the-loop)

---

## ğŸ—ï¸ Architecture Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User       â”‚
â”‚   Query      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LangGraph Application                 â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Retrieve    â”‚â”€â”€â”€â”€â”€â–¶â”‚  Format      â”‚       â”‚
â”‚  â”‚  Node        â”‚      â”‚  Context     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Generate    â”‚â—€â”€â”€â”€â”€â”€â”‚  Generate    â”‚       â”‚
â”‚  â”‚  Response    â”‚      â”‚  Node       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB    â”‚    â”‚   Ollama     â”‚
â”‚  (Vector DB) â”‚    â”‚   (LLM)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **User Input** â†’ Chat interface receives query
2. **Retrieve Node** â†’ 
   - Convert query to embedding (using existing `text_to_embeddings`)
   - Search ChromaDB (using existing `ChromaDBService.read()`)
   - Return top-k relevant documents
3. **Format Context Node** â†’
   - Combine retrieved documents
   - Build prompt template
   - Include conversation history (if multi-turn)
4. **Generate Node** â†’
   - Call Ollama LLM API (new: chat completion endpoint)
   - Pass formatted prompt
   - Get generated response
5. **Respond Node** â†’
   - Format response
   - Return to user
   - Update conversation history

---

## ğŸ“‹ Implementation Plan

### Phase 1: Setup & Dependencies
- [ ] Install LangGraph and LangChain dependencies
- [ ] Set up Ollama chat model (not just embeddings)
- [ ] Create project structure
- [ ] Update `config.py` with LLM settings

### Phase 2: Core RAG Components
- [ ] Create `retrievers/` module with ChromaDB retriever
- [ ] Create `prompts/` module with prompt templates
- [ ] Create `llm/` module for Ollama chat integration
- [ ] Create utility functions for context formatting

### Phase 3: LangGraph Implementation
- [ ] Define LangGraph state schema
- [ ] Create graph nodes (retrieve, format, generate, respond)
- [ ] Define graph edges and conditional routing
- [ ] Build and compile the graph

### Phase 4: Chat Interface
- [ ] Create simple CLI chat interface
- [ ] Add conversation history management
- [ ] Implement streaming responses (optional)
- [ ] Add error handling

### Phase 5: Testing & Refinement
- [ ] Test with various queries
- [ ] Add response validation
- [ ] Implement fallback mechanisms
- [ ] Add logging and observability

---

## ğŸ“ File Structure

```
embeddings-py/
â”œâ”€â”€ config.py                          # âœ… Existing
â”œâ”€â”€ db/
â”‚   â””â”€â”€ chromadb_service.py            # âœ… Existing
â”œâ”€â”€ utils.py                           # âœ… Existing
â”œâ”€â”€ mock-data/
â”‚   â””â”€â”€ payment_support_data.json      # âœ… Existing
â”‚
â”œâ”€â”€ rag/                               # ğŸ†• New RAG module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retriever.py                   # ChromaDB retriever wrapper
â”‚   â”œâ”€â”€ prompts.py                     # Prompt templates
â”‚   â””â”€â”€ context_formatter.py           # Context formatting utilities
â”‚
â”œâ”€â”€ llm/                               # ğŸ†• LLM integration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ollama_chat.py                 # Ollama chat completion client
â”‚
â”œâ”€â”€ langgraph_app/                     # ğŸ†• LangGraph application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ state.py                       # State schema definition
â”‚   â”œâ”€â”€ nodes.py                       # Graph nodes (functions)
â”‚   â”œâ”€â”€ graph.py                       # Graph definition and compilation
â”‚   â””â”€â”€ chat.py                        # Chat interface
â”‚
â”œâ”€â”€ requirements.txt                   # Update with new dependencies
â””â”€â”€ README_RAG.md                      # ğŸ†• RAG documentation
```

---

## ğŸ“¦ Dependencies

### New Dependencies to Add

```txt
# Existing dependencies (keep these)
chromadb
requests
numpy

# New dependencies for RAG + LangGraph
langgraph>=0.2.0          # Core LangGraph framework
langchain>=0.3.0          # LangChain integration (used by LangGraph)
langchain-community>=0.3.0 # Community integrations
langchain-core>=0.3.0     # Core LangChain abstractions
```

### Optional (for better UX)
```txt
rich>=13.0.0              # Beautiful terminal output
typer>=0.9.0              # CLI framework (if building CLI)
```

---

## ğŸ”¨ Step-by-Step Implementation

### Step 1: Update Configuration (`config.py`)

Add LLM configuration:
```python
# Existing embedding config...
# Add:

# Ollama Chat Configuration
OLLAMA_CHAT_API_URL = "http://localhost:11434/api/chat"
CHAT_MODEL = "llama3.2"  # or "mistral", "qwen2", etc. (any chat model)
CHAT_TIMEOUT = 30  # seconds

# RAG Configuration
RETRIEVAL_TOP_K = 3  # Number of documents to retrieve
MAX_CONTEXT_LENGTH = 2000  # Max characters in context
ENABLE_CONVERSATION_HISTORY = True
MAX_HISTORY_LENGTH = 5  # Number of previous exchanges to keep
```

### Step 2: Create Retriever Module (`rag/retriever.py`)

**Purpose**: Wrap ChromaDB search in a clean interface for RAG

**Key Functions**:
- `retrieve_relevant_docs(query: str, top_k: int) -> List[Dict]`
  - Uses existing `ChromaDBService` and `text_to_embeddings`
  - Returns list of documents with metadata and similarity scores

**Implementation Notes**:
- Reuse `ChromaDBService.read(query_texts=[query], n_results=top_k)`
- Format results: `{text, metadata, score, id}`
- Filter by similarity threshold (optional)

### Step 3: Create LLM Chat Client (`llm/ollama_chat.py`)

**Purpose**: Interface with Ollama's chat completion API

**Key Functions**:
- `generate_response(messages: List[Dict]) -> str`
  - Send messages to Ollama chat API
  - Return generated text
  - Handle errors and timeouts

**Message Format**:
```python
messages = [
    {"role": "system", "content": "You are a helpful payment support assistant."},
    {"role": "user", "content": "What is my daily limit?"}
]
```

### Step 4: Create Prompt Templates (`rag/prompts.py`)

**Purpose**: Define how to format prompts for the LLM

**Key Templates**:
- `SYSTEM_PROMPT`: Instructions for the assistant
- `CONTEXT_PROMPT`: How to format retrieved context
- `USER_QUERY_PROMPT`: How to format user query

**Example Template**:
```python
SYSTEM_PROMPT = """You are a helpful payment support assistant. 
Answer questions based ONLY on the provided context. 
If the context doesn't contain the answer, say "I don't have that information."
"""

CONTEXT_FORMAT = """Context from knowledge base:
{context}
"""
```

### Step 5: Define LangGraph State (`langgraph_app/state.py`)

**Purpose**: Define the shared state structure

**State Schema**:
```python
from typing import TypedDict, List, Dict, Annotated
from langgraph.graph.message import add_messages

class GraphState(TypedDict):
    messages: Annotated[List[Dict], add_messages]  # Conversation history
    query: str                                      # Current user query
    retrieved_docs: List[Dict]                     # Retrieved documents
    context: str                                    # Formatted context
    response: str                                   # Generated response
    metadata: Dict                                  # Additional metadata
```

### Step 6: Create Graph Nodes (`langgraph_app/nodes.py`)

**Purpose**: Implement each step of the RAG pipeline

**Nodes**:

1. **`retrieve_node(state: GraphState) -> GraphState`**
   - Extract query from state
   - Call retriever to get relevant docs
   - Update `state["retrieved_docs"]`

2. **`format_context_node(state: GraphState) -> GraphState`**
   - Take retrieved docs
   - Format using prompt template
   - Update `state["context"]`

3. **`generate_node(state: GraphState) -> GraphState`**
   - Build messages: system + context + user query + history
   - Call LLM
   - Update `state["response"]`

4. **`respond_node(state: GraphState) -> GraphState`**
   - Format final response
   - Add to message history
   - Return state

### Step 7: Build the Graph (`langgraph_app/graph.py`)

**Purpose**: Connect nodes and define flow

**Graph Structure**:
```
START â†’ retrieve_node â†’ format_context_node â†’ generate_node â†’ respond_node â†’ END
```

**Implementation**:
```python
from langgraph.graph import StateGraph

def create_graph():
    graph = StateGraph(GraphState)
    
    # Add nodes
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("format_context", format_context_node)
    graph.add_node("generate", generate_node)
    graph.add_node("respond", respond_node)
    
    # Define edges
    graph.set_entry_point("retrieve")
    graph.add_edge("retrieve", "format_context")
    graph.add_edge("format_context", "generate")
    graph.add_edge("generate", "respond")
    graph.set_finish_point("respond")
    
    return graph.compile()
```

### Step 8: Create Chat Interface (`langgraph_app/chat.py`)

**Purpose**: User-facing chat interface

**Features**:
- Simple CLI loop: `while True: query = input("You: ")`
- Initialize graph with empty state
- Invoke graph with user query
- Display response
- Handle exit commands (`/quit`, `/exit`)

**Example Flow**:
```python
graph = create_graph()

while True:
    query = input("\nYou: ").strip()
    if query.lower() in ["/quit", "/exit"]:
        break
    
    # Invoke graph
    result = graph.invoke({
        "messages": [],
        "query": query,
        "retrieved_docs": [],
        "context": "",
        "response": "",
        "metadata": {}
    })
    
    print(f"\nAssistant: {result['response']}")
```

### Step 9: Add Conversation History (Enhancement)

**Purpose**: Support multi-turn conversations

**Changes**:
- Update state initialization to preserve previous messages
- Modify `respond_node` to append to message history
- Update `generate_node` to include conversation history in prompt

### Step 10: Add Error Handling & Validation

**Purpose**: Make the app robust

**Add**:
- Try-catch in each node
- Validation node (check if retrieval found docs)
- Fallback node (if retrieval fails, use general response)
- Logging for debugging

---

## ğŸ§ª Testing Strategy

### Unit Tests
- Test retriever with various queries
- Test prompt formatting
- Test LLM client with mock responses
- Test individual nodes in isolation

### Integration Tests
- Test full RAG pipeline end-to-end
- Test with different query types
- Test conversation history
- Test error scenarios

### Manual Testing Queries
1. **Direct match**: "What is my daily transaction limit?"
2. **Semantic match**: "How much can I spend per day?" (should retrieve limit info)
3. **Out of scope**: "What's the weather?" (should say it doesn't have that info)
4. **Multi-turn**: 
   - User: "Tell me about card blocking"
   - User: "How do I do that?" (should understand "that" = blocking)

---

## ğŸ“ Learning Objectives

By implementing this, you'll learn:

1. **RAG Fundamentals**:
   - How retrieval works with embeddings
   - How to augment prompts with context
   - How LLMs use context to generate answers

2. **LangGraph Concepts**:
   - State management in graph-based applications
   - Node-based architecture
   - Conditional routing and control flow

3. **Production Patterns**:
   - Modular code organization
   - Error handling in LLM applications
   - Conversation management
   - Prompt engineering

4. **Integration Skills**:
   - Connecting vector databases with LLMs
   - Building end-to-end AI applications
   - Debugging multi-step pipelines

---

## ğŸš€ Next Steps After Implementation

1. **Enhance Retrieval**:
   - Add re-ranking
   - Implement hybrid search (keyword + semantic)
   - Add metadata filtering

2. **Improve Generation**:
   - Add response streaming
   - Implement confidence scoring
   - Add citation of sources

3. **Add Features**:
   - Web interface (Streamlit/Gradio)
   - User feedback collection
   - Analytics and monitoring

4. **Optimize**:
   - Caching frequently asked questions
   - Batch processing
   - Performance tuning

---

## ğŸ“ Notes

- Start simple: Get basic RAG working first, then add complexity
- Use existing code: Reuse `ChromaDBService` and `text_to_embeddings`
- Test incrementally: Test each node before building the full graph
- Document as you go: Add docstrings explaining RAG concepts
- Experiment: Try different prompt templates and retrieval strategies

---

## âœ… Checklist Before Starting Implementation

- [ ] Ollama is running with a chat model (not just embedding model)
- [ ] ChromaDB has embeddings stored (run `payment_support_embeddings.py`)
- [ ] Python 3.8+ is installed
- [ ] Virtual environment is set up (recommended)
- [ ] You understand the existing codebase structure

---

**Ready to build? Start with Phase 1 and work through each step methodically!** ğŸ¯

---

## ğŸ”„ Complete Flow Diagram: Final Product

### End-to-End System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER INTERFACE (CLI)                               â”‚
â”‚                                                                              â”‚
â”‚   User types: "What is my daily transaction limit?"                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGGRAPH APPLICATION (Main Entry)                        â”‚
â”‚                                                                              â”‚
â”‚   chat.py: main()                                                           â”‚
â”‚   â”œâ”€ Initialize GraphState                                                  â”‚
â”‚   â””â”€ graph.invoke(initial_state)                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LANGGRAPH STATE INITIALIZATION                      â”‚
â”‚                                                                              â”‚
â”‚   GraphState = {                                                            â”‚
â”‚     "messages": [],                    # Empty conversation history        â”‚
â”‚     "query": "What is my daily...",     # User query                       â”‚
â”‚     "retrieved_docs": [],              # Will be populated                â”‚
â”‚     "context": "",                     # Will be formatted                â”‚
â”‚     "response": "",                    # Will be generated                â”‚
â”‚     "metadata": {}                      # Additional info                  â”‚
â”‚   }                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         NODE 1: RETRIEVE NODE                                â”‚
â”‚                    (langgraph_app/nodes.py: retrieve_node)                  â”‚
â”‚                                                                              â”‚
â”‚   Input State:                                                               â”‚
â”‚     - query: "What is my daily transaction limit?"                         â”‚
â”‚                                                                              â”‚
â”‚   Process:                                                                   â”‚
â”‚   1. Extract query from state                                              â”‚
â”‚   2. Call rag/retriever.py: retrieve_relevant_docs()                      â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â”œâ”€â†’ utils.py: text_to_embeddings([query])                             â”‚
â”‚      â”‚   â”‚                                                                  â”‚
â”‚      â”‚   â””â”€â†’ Ollama API: POST /api/embed                                    â”‚
â”‚      â”‚       â”‚                                                               â”‚
â”‚      â”‚       â””â”€â†’ Returns: [0.123, -0.456, ..., 0.789] (embedding vector)   â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â””â”€â†’ db/chromadb_service.py: read(query_texts=[query], n_results=3)    â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â””â”€â†’ ChromaDB: Vector Similarity Search                            â”‚
â”‚              â”‚                                                               â”‚
â”‚              â””â”€â†’ Returns: Top 3 similar documents with scores                â”‚
â”‚                                                                              â”‚
â”‚   Output State:                                                              â”‚
â”‚     - retrieved_docs: [                                                     â”‚
â”‚         {                                                                   â”‚
â”‚           "text": "Your daily transaction limit...",                       â”‚
â”‚           "metadata": {"category": "transaction_limits"},                   â”‚
â”‚           "score": 0.85,                                                   â”‚
â”‚           "id": "support_001"                                              â”‚
â”‚         },                                                                 â”‚
â”‚         ... (2 more docs)                                                  â”‚
â”‚       ]                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NODE 2: FORMAT CONTEXT NODE                               â”‚
â”‚              (langgraph_app/nodes.py: format_context_node)                  â”‚
â”‚                                                                              â”‚
â”‚   Input State:                                                               â”‚
â”‚     - retrieved_docs: [3 documents with text, metadata, scores]            â”‚
â”‚                                                                              â”‚
â”‚   Process:                                                                   â”‚
â”‚   1. Extract retrieved_docs from state                                     â”‚
â”‚   2. Call rag/context_formatter.py: format_context()                       â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â”œâ”€â†’ rag/prompts.py: CONTEXT_FORMAT template                           â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â””â”€â†’ Combine documents:                                                â”‚
â”‚          """                                                                â”‚
â”‚          Context from knowledge base:                                       â”‚
â”‚                                                                             â”‚
â”‚          [1] Category: transaction_limits                                   â”‚
â”‚          Your daily transaction limit depends on your account tier...      â”‚
â”‚                                                                             â”‚
â”‚          [2] Category: transaction_limits                                   â”‚
â”‚          ... (more context)                                                â”‚
â”‚          """                                                                â”‚
â”‚                                                                              â”‚
â”‚   Output State:                                                              â”‚
â”‚     - context: "Context from knowledge base:\n[1] Category: ..."           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NODE 3: GENERATE NODE                                  â”‚
â”‚                 (langgraph_app/nodes.py: generate_node)                     â”‚
â”‚                                                                              â”‚
â”‚   Input State:                                                               â”‚
â”‚     - query: "What is my daily transaction limit?"                         â”‚
â”‚     - context: "Context from knowledge base: ..."                           â”‚
â”‚     - messages: [] (or previous conversation history)                       â”‚
â”‚                                                                              â”‚
â”‚   Process:                                                                   â”‚
â”‚   1. Build messages array:                                                  â”‚
â”‚      [                                                                      â”‚
â”‚        {                                                                    â”‚
â”‚          "role": "system",                                                  â”‚
â”‚          "content": "You are a helpful payment support assistant..."        â”‚
â”‚        },                                                                   â”‚
â”‚        {                                                                    â”‚
â”‚          "role": "user",                                                    â”‚
â”‚          "content": "Context: ...\n\nQuestion: What is my daily limit?"     â”‚
â”‚        }                                                                    â”‚
â”‚      ]                                                                      â”‚
â”‚                                                                              â”‚
â”‚   2. Call llm/ollama_chat.py: generate_response(messages)                  â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â””â”€â†’ Ollama API: POST /api/chat                                         â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â”‚ Request:                                                         â”‚
â”‚          â”‚ {                                                                â”‚
â”‚          â”‚   "model": "llama3.2",                                          â”‚
â”‚          â”‚   "messages": [                                                  â”‚
â”‚          â”‚     {"role": "system", "content": "..."},                       â”‚
â”‚          â”‚     {"role": "user", "content": "..."}                          â”‚
â”‚          â”‚   ]                                                              â”‚
â”‚          â”‚ }                                                                â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â””â”€â†’ Response:                                                     â”‚
â”‚              {                                                              â”‚
â”‚                "message": {                                                 â”‚
â”‚                  "content": "Your daily transaction limit depends on..."   â”‚
â”‚                }                                                            â”‚
â”‚              }                                                              â”‚
â”‚                                                                              â”‚
â”‚   Output State:                                                              â”‚
â”‚     - response: "Your daily transaction limit depends on your account..."  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         NODE 4: RESPOND NODE                                 â”‚
â”‚                  (langgraph_app/nodes.py: respond_node)                     â”‚
â”‚                                                                              â”‚
â”‚   Input State:                                                               â”‚
â”‚     - response: "Your daily transaction limit depends on..."               â”‚
â”‚     - query: "What is my daily transaction limit?"                         â”‚
â”‚     - messages: [] (previous history)                                      â”‚
â”‚                                                                              â”‚
â”‚   Process:                                                                   â”‚
â”‚   1. Format final response                                                 â”‚
â”‚   2. Update conversation history:                                          â”‚
â”‚      messages.append({"role": "user", "content": query})                   â”‚
â”‚      messages.append({"role": "assistant", "content": response})          â”‚
â”‚   3. Prepare final state                                                    â”‚
â”‚                                                                              â”‚
â”‚   Output State:                                                              â”‚
â”‚     - response: "Your daily transaction limit depends on..."                â”‚
â”‚     - messages: [                                                          â”‚
â”‚         {"role": "user", "content": "What is my daily..."},                â”‚
â”‚         {"role": "assistant", "content": "Your daily..."}                  â”‚
â”‚       ]                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LANGGRAPH COMPLETION                                 â”‚
â”‚                                                                              â”‚
â”‚   Graph execution complete                                                  â”‚
â”‚   Final state returned to chat.py                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CHAT INTERFACE OUTPUT                               â”‚
â”‚                                                                              â”‚
â”‚   chat.py extracts response from state                                      â”‚
â”‚   Prints: "Assistant: Your daily transaction limit depends on..."          â”‚
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚ You: What is my daily transaction limit?              â”‚               â”‚
â”‚   â”‚                                                        â”‚               â”‚
â”‚   â”‚ Assistant: Your daily transaction limit depends on    â”‚               â”‚
â”‚   â”‚            your account tier. Basic accounts have a    â”‚               â”‚
â”‚   â”‚            limit of Â£1,000 per day, Premium accounts   â”‚               â”‚
â”‚   â”‚            have Â£5,000 per day, and Metal accounts     â”‚               â”‚
â”‚   â”‚            have Â£10,000 per day.                      â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                              â”‚
â”‚   Loop continues: Waiting for next user input...                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          COMPLETE SYSTEM ARCHITECTURE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    USER      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. User Query
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LANGGRAPH APPLICATION                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         GRAPH EXECUTION                               â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”‚
â”‚  â”‚  â”‚   RETRIEVE   â”‚â”€â”€â”€â–¶â”‚   FORMAT     â”‚â”€â”€â”€â–¶â”‚   GENERATE   â”‚          â”‚  â”‚
â”‚  â”‚  â”‚    NODE      â”‚    â”‚   CONTEXT    â”‚    â”‚    NODE      â”‚          â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚  â”‚         â”‚                                        â”‚                   â”‚  â”‚
â”‚  â”‚         â”‚                                        â”‚                   â”‚  â”‚
â”‚  â”‚         â–¼                                        â–¼                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”‚
â”‚  â”‚  â”‚   RESPOND    â”‚                        â”‚   STATE      â”‚          â”‚  â”‚
â”‚  â”‚  â”‚    NODE      â”‚                        â”‚   MANAGER    â”‚          â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                                           â”‚
        â”‚ 2. Query Embedding                                       â”‚ 5. Chat Request
        â”‚    (text_to_embeddings)                                  â”‚    (generate_response)
        â”‚                                                           â”‚
        â–¼                                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CHROMADB   â”‚                                            â”‚    OLLAMA    â”‚
â”‚              â”‚                                            â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Vector â”‚ â”‚                                            â”‚  â”‚  LLM   â”‚  â”‚
â”‚  â”‚  Store â”‚ â”‚                                            â”‚  â”‚  Chat  â”‚  â”‚
â”‚  â”‚        â”‚ â”‚                                            â”‚  â”‚  Model â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                                            â”‚              â”‚
â”‚  3. Semanticâ”‚                                            â”‚  6. Responseâ”‚
â”‚     Search  â”‚                                            â”‚     Text     â”‚
â”‚             â”‚                                            â”‚              â”‚
â”‚  4. Top-K   â”‚                                            â”‚              â”‚
â”‚     Docs    â”‚                                            â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                                           â”‚
        â”‚                                                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ 7. Final Response
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     USER     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚  â”‚  LangGraph   â”‚  â”‚ ChromaDB â”‚  â”‚   Ollama     â”‚  â”‚  User    â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚ Query   â”‚â”€â–¶â”‚ Retrieve     â”‚â”€â–¶â”‚ Search   â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚â—€â”€â”‚ Results  â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚ Format       â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚ Context      â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚ Generate     â”‚  â”‚          â”‚â”€â–¶â”‚ Chat API     â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚â—€â”€â”‚ Response     â”‚  â”‚          â”‚
â”‚         â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚  â”‚ Respond      â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚         â”‚â—€â”€â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â”‚ Answer  â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Evolution Through Nodes

```
Initial State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                            â”‚
â”‚   "messages": [],                                           â”‚
â”‚   "query": "What is my daily transaction limit?",          â”‚
â”‚   "retrieved_docs": [],                                     â”‚
â”‚   "context": "",                                            â”‚
â”‚   "response": "",                                           â”‚
â”‚   "metadata": {}                                            â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
After RETRIEVE Node:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                            â”‚
â”‚   "messages": [],                                           â”‚
â”‚   "query": "What is my daily transaction limit?",          â”‚
â”‚   "retrieved_docs": [                                       â”‚
â”‚     {"text": "...", "metadata": {...}, "score": 0.85},    â”‚
â”‚     ...                                                     â”‚
â”‚   ],                                                        â”‚
â”‚   "context": "",                                            â”‚
â”‚   "response": "",                                           â”‚
â”‚   "metadata": {}                                            â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
After FORMAT_CONTEXT Node:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                            â”‚
â”‚   "messages": [],                                           â”‚
â”‚   "query": "What is my daily transaction limit?",          â”‚
â”‚   "retrieved_docs": [...],                                  â”‚
â”‚   "context": "Context from knowledge base:\n[1] Category...",â”‚
â”‚   "response": "",                                           â”‚
â”‚   "metadata": {}                                            â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
After GENERATE Node:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                            â”‚
â”‚   "messages": [],                                           â”‚
â”‚   "query": "What is my daily transaction limit?",          â”‚
â”‚   "retrieved_docs": [...],                                  â”‚
â”‚   "context": "...",                                         â”‚
â”‚   "response": "Your daily transaction limit depends on...", â”‚
â”‚   "metadata": {}                                            â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
After RESPOND Node (Final State):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                                                            â”‚
â”‚   "messages": [                                            â”‚
â”‚     {"role": "user", "content": "What is my daily..."},   â”‚
â”‚     {"role": "assistant", "content": "Your daily..."}      â”‚
â”‚   ],                                                        â”‚
â”‚   "query": "What is my daily transaction limit?",          â”‚
â”‚   "retrieved_docs": [...],                                  â”‚
â”‚   "context": "...",                                         â”‚
â”‚   "response": "Your daily transaction limit depends on...", â”‚
â”‚   "metadata": {"retrieval_count": 3, "model": "llama3.2"}  â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Turn Conversation Flow

```
Turn 1:
User: "What is my daily transaction limit?"
  â†’ Retrieve â†’ Format â†’ Generate â†’ Respond
  â†’ Response: "Your daily limit depends on your account tier..."

Turn 2:
User: "How do I check it?"
  â†’ State includes previous messages
  â†’ Retrieve (may find related docs about checking limits)
  â†’ Format (includes conversation context)
  â†’ Generate (LLM understands "it" = transaction limit)
  â†’ Respond: "You can check your current limit in the app..."

Turn 3:
User: "What about card blocking?"
  â†’ New topic, retrieves card management docs
  â†’ Format â†’ Generate â†’ Respond
  â†’ Response: "To block your card, go to the Cards section..."
```

---

**This diagram shows the complete end-to-end flow of the RAG + LangGraph chat support application!** ğŸ¯

